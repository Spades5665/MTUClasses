{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591ab9e9-5501-431d-9296-9f9db1b5ed9a",
   "metadata": {},
   "source": [
    "# a4 - Python\n",
    "\n",
    "This assignment will cover topics of text mining and clustering\n",
    "\n",
    "Make sure that you keep this notebook named as \"a4.ipynb\" \n",
    "\n",
    "Any other packages or tools, outside those listed in the assignments or Canvas, should be cleared\n",
    "by Dr. Brown before use in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b64615-6e16-4c36-a3e5-9ebd4fd53bc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Q0 - Setup\n",
    "\n",
    "The following code looks to see whether your notebook is run on Gradescope (GS), Colab (COLAB), or the linux Python environment you were asked to setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b79a14-abb4-482a-a14a-f2e68c24177f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: GS - False, COLAB - False, LLM - True\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "import platform \n",
    "import sys \n",
    "\n",
    "# flag if notebook is running on Gradescope \n",
    "if re.search(r'am', platform.uname().release): \n",
    "    GS = True\n",
    "else: \n",
    "    GS = False\n",
    "\n",
    "# flag if notebook is running on Colaboratory \n",
    "try:\n",
    "  import google.colab\n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False\n",
    "\n",
    "# flag if running on Linux lab machines. \n",
    "cname = platform.uname().node\n",
    "if re.search(r'(guardian|colossus|c28|coc-15954-m)', cname):\n",
    "    LLM = True \n",
    "else: \n",
    "    LLM = False\n",
    "\n",
    "print(\"System: GS - %s, COLAB - %s, LLM - %s\" % (GS, COLAB, LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff61a4a-0c1b-43f0-9bd4-12567457e274",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Notebook Setup\n",
    "\n",
    "It is good practice to list all imports needed at the top of the notebook. You can import modules in later cells as needed, but listing them at the top clearly shows all which are needed to be available / installed.\n",
    "\n",
    "If you are doing development on Colab, the otter-grader package is not available, so you will need to install it with pip (uncomment the cell directly below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae89d29-3cd1-404b-8c4a-efc67131cef9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Only uncomment if you developing on Colab \n",
    "# if COLAB == True: \n",
    "#     print(\"Installing otter:\")\n",
    "#     !pip install otter-grader==4.2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ae8298-76c6-444d-9404-b25c41e67abe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Import standard DS packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy\n",
    "import statistics\n",
    "import textwrap\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree        # decision tree classifier\n",
    "from sklearn import neighbors   # knn classifier\n",
    "from sklearn import naive_bayes # naive bayes classifier \n",
    "from sklearn import svm         # svm classifier\n",
    "from sklearn import ensemble    # ensemble classifiers\n",
    "from sklearn import metrics     # performance evaluation metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn import cluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Package for Autograder \n",
    "import otter \n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47808e2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q0</pre></strong> passed! 💯</p>"
      ],
      "text/plain": [
       "q0 results: All test cases passed!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac0f14-3f7f-48fc-8127-fb4400c04cb5",
   "metadata": {},
   "source": [
    "# Q1 - Text Classification \n",
    "\n",
    "\n",
    "You will look to predict whether scenes in Shakespeare's plays come from the comedies or histories.  Shakespeare's comedies include plays such as: The Taming of the Shrew, The Merchant of Venice, Much Ado About Nothing, and more.  The histories include: Richard II, Richard III, Henry IV part 1, Henry IV part 2, Henry V, Henry VI (part 1-3). \n",
    "\n",
    "The plays were downloaded from the [Shakespeare Corpus](http://hdl.handle.net/11040/24448).  Note, the original plays were downloaded from [Project Gutenberg](https://www.gutenberg.org/). \n",
    "\n",
    "Note, the plays have already had significant preprocessing.  The plays have been scrubbed by: removing digits, making the file all lowercase, and removing punctuation, excluding hyphens and word-internal apostrophes. Also, the character names and stage directions have been removed manually. An example of the text would be like this:\n",
    "\n",
    "*Before scrubbing:*\n",
    "\n",
    "    ADAM. Yonder comes my master, your brother.\n",
    "    ORLANDO. Go apart, Adam, and thou shalt hear how he will shake me\n",
    "    up. [ADAM retires]\n",
    "    OLIVER. Now, sir! what make you here?\n",
    "\n",
    "*After scrubbing:*\n",
    "\n",
    "    yonder comes my master your brother\n",
    "    go apart adam and thou shalt hear how he will shake me\n",
    "    up\n",
    "    now sir what make you here\n",
    "\n",
    "The text files are split into negative - comedies and positive - histories.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24bb34-5d30-469b-8506-af086ef26663",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(a) - Load the Data \n",
    "\n",
    "Load the plays into a list `textdata` and a np.ndarray `yvalues`.  I highly suggest using `scikit-learn`'s `load_files` function, with the `random_state` set to 42.  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38572ccd-0ce4-4994-b9d2-7a279e6effda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'data/shakespeare/histories/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the plays data     \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plays \u001b[38;5;241m=\u001b[39m \u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/shakespeare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract text data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m textdata \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/.conda/envs/cs4821/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/cs4821/lib/python3.10/site-packages/sklearn/datasets/_base.py:281\u001b[0m, in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[0m\n\u001b[1;32m    279\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[0;32m--> 281\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     data \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mdecode(encoding, decode_error) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/.conda/envs/cs4821/lib/python3.10/pathlib.py:1126\u001b[0m, in \u001b[0;36mPath.read_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.conda/envs/cs4821/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'data/shakespeare/histories/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "# Load the plays data     \n",
    "plays = load_files(\"data/shakespeare\", random_state=42)\n",
    "\n",
    "# Extract text data\n",
    "textdata = dataset.data\n",
    "\n",
    "# Extract target values\n",
    "yvalues = dataset.target\n",
    "\n",
    "print(\"Samples per class: {}\".format(np.bincount(yvalues)))\n",
    "\n",
    "plays.filenames[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d4ff7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45687d48-dc6f-42bd-8989-b682b470b375",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(b) - Prepare the Data \n",
    "\n",
    "Split the data into `text_trainval`, `text_test` and `y_trainval`, `y_test` variables.  Use 20% of the data in the test set with a `random_state` of 42 and make sure to stratify the split (the data is imbalanced). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f1265-1dc8-4754-af74-ce34ae6de560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7998e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3cafb-6d39-4c4c-b3bf-1b540c984e8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(c)  - Explore the Data\n",
    "\n",
    "Create a document-term count matrix for the \"trainval\" data using the default tokenizer, removing the standard English stopwords and store this in `dtm_trainval`.\n",
    "\n",
    "Store the names of the terms in the dtm matrix in the variable `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e2164-9fab-4e1c-aa90-59bb1c77a426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create document-term count matrix for the \"trainval\" text data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecace364",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fad03-d191-41b5-bc71-c2ff3f0b89c6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q1(d) - Explore the Data \n",
    "\n",
    "Create a plot showing the top 15 most frequently used words in the trainval text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b1858-6920-4405-a383-0ac7e6ed41f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot of the top 15 most frequently used words \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dbbe1-5d9c-4a29-9f2e-834c1ee7752d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q1(e) - Explore the Data \n",
    "\n",
    "For the trainval text data, plot the top 15 most frequently used words in the histories and the comedies.  Put these two bar plots side-by-side to compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b0c96-3f27-41c4-bbc0-3a5bda27868c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot of the top 15 most frequently used words in the \n",
    "#  Comedies and Histories. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c4ac8-8769-4b38-a4cf-7c9b8a561f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1(f) - Bernoulli Naive Bayes \n",
    "\n",
    "Let's know explore using Bernoulli Naive Bayes as a classifier, `bern_nb`, to predict the type of play. \n",
    "\n",
    "We will use the split of the data into trainval / test found above to train the model and then evaluate it's performance. \n",
    "\n",
    "Create the training data, `X_trainval` to be binary with features using the default tokenizer, stop words removed, appear in at least 5 documents and is limited to the top 5000 features.  \n",
    "\n",
    "Calculate the training accuracy `train_acc_bern` and testing accuracy `test_acc_bern` for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5153fbb-3e26-4f26-ad50-087a8d575269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Bernoulli Naive Bayes model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6ead8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7cf4f-3c21-45bd-b7e3-1e6c391a8ef1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(g) - Multinomial Naive Bayes \n",
    "\n",
    "Let's know explore using multinomial Naive Bayes as a classifier, `mult_nb`, to predict the type of play. \n",
    "\n",
    "We will use the split of the data into trainval / test found above to train the model and then evaluate it's performance. \n",
    "\n",
    "Create the training data, `X_trainval` with features using the default tokenizer, stop words removed, appear in at least 5 documents and is limited to the top 5000 features.  \n",
    "\n",
    "Calculate the training accuracy `train_acc_mult` and testing accuracy `test_acc_mult` for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4725275-bcae-47a4-940e-c58c84ba1dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Multinomial Naive Bayes model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f6205",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145008d-5c07-412f-b848-bce0a7ddaa8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q1(h) - Naive Bayes Models \n",
    "\n",
    "Looking at the results of the two models above. Answer the following questions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c795a40-e5d6-4dcf-924d-afff286c1a26",
   "metadata": {},
   "source": [
    "Which of the two models is preferred?  Why?   (10 words or less)\n",
    "\n",
    "\n",
    "What is a problem for both models?  How might you solve it? (12 words or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96621bc4-e895-401f-9835-de3ed9225f6f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1(i) - Other Models \n",
    "\n",
    "Let's now look to explore using other models. \n",
    "\n",
    "You will set up a pipeline, `pipe`, that will use a Random Forest model with 100 trees and a `random_state` = 42.  \n",
    "\n",
    "In the pipeline (`param_grid`), you will consider using both a document term count matrix as well as a TF-IDF matrix. \n",
    "In either case, limit the matrix to words that appear in at least 5 documents and remove English stop words.  Consider features of unigrams, unigrams + bigrams, and bigrams.  Examine a maximum feature limit of either 2500 or 5000.  \n",
    "\n",
    "Optimize your choice of hyperparameters using GridSearchCV, `grid`, with stratified 5-fold cross-validation (random_state = 42), select the parameters using AUC. (See how to set up the scorer below)\n",
    "\n",
    "Note, do not run the jobs in parallel, you may exceed the memory resources of the autograder on Gradescope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4cc4d-133f-40e8-8885-adcd49d22eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Pipeline to find best model\n",
    "\n",
    "pipe = ...\n",
    "\n",
    "param_grid = ...\n",
    "\n",
    "cvStrat = ...\n",
    "\n",
    "score_fn = metrics.make_scorer(metrics.roc_auc_score, needs_threshold = False)\n",
    "grid = ...\n",
    "\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab3923",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff9694-2fc6-4182-961d-51d0f46a711f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(j) - Explore the Results \n",
    "\n",
    "Calculate the AUC on the test text, `auc_test`.   \n",
    "\n",
    "Gather the importances of the features in the best model in `importance`. [Feature Importance Example](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "\n",
    "Create a bar plot with the top 10 features sorted by importance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6a0de-2099-4778-bedc-01120547ce36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate performance on test data `auc_test` \n",
    "\n",
    "# Create plot of top 10 features sorted by importance. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52243f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc76d5c-6736-4473-94b0-fa4461ffbeaa",
   "metadata": {
    "id": "o3ZuWT4INkFu"
   },
   "source": [
    "# Q2\n",
    "\n",
    "Consider methods to cluster NBA players based on their statistics. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e03efe-dadd-4f43-b151-20f54ba5a45a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q2(a) \n",
    "\n",
    "Load in data for NBA players from the 2018-2019 season. \n",
    "\n",
    "Filter the players to only consider those who have played in more than 20 games.  \n",
    "\n",
    "Ignore the first 7 columns as well as ignore columns of statistics with percentages (FG%, 3P%, 2P%, eFG%, FT%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b566d0b-cae7-422a-8b64-54bf0f0839df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in data and filter out requested rows and columns. \n",
    "\n",
    "nba = ...\n",
    "\n",
    "\n",
    "\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb6e8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2743e23-a779-40e1-97e0-98c32e46d449",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nQuPSHv_bmW1"
   },
   "source": [
    "## Q2(b)\n",
    "\n",
    "The features have different ranges, therefore we should scale the data before considering the clustering analysis. Scale the data using min-max normalization with range of [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b299f-e289-4d2f-9ce5-a693f353aa2f",
   "metadata": {
    "id": "sHdWv_KnSXcO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data \n",
    "\n",
    "scaler = ...\n",
    "nbaScaled = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e46944",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54a32c-8890-4e47-87f9-db3c7ea6814d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zRQAkQN_bjP-"
   },
   "source": [
    "## Q2(c)\n",
    "\n",
    "Run Kmeans clustering on the data with k=2, . . . , 12.  Set the `random_state` in the Kmeans method to 42 and `n_init` to 10. \n",
    "For each value of k, keep track of the within-cluster variation (this quantity is referred to as different terms such as “inertia” and total “within-cluster sum-of-squares”), the Calinski-Harabasz score, and the Davies-Bouldin index on the resulting clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec112415-4bca-4942-9b73-3cc8774e12d6",
   "metadata": {
    "id": "FlDI1-jTS2kb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Kmeans \n",
    "\n",
    "sse = []\n",
    "dbscore = []\n",
    "chscore = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fd506",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f61a8a-d400-4712-a373-d779c4366a3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3KxomcXvHzWv"
   },
   "source": [
    "## Q2(d)\n",
    "\n",
    "Assuming the best number of clusters is 4 (depending on which measure we use different number of clusters is preferred with this data). \n",
    "\n",
    "Run Kmeans again with this value for $k$ (use `n_init` = 10 and `random_state` = 10).\n",
    "\n",
    "Create a DataFrame `clusterStats` with the mean statistics (centers) of each group. \n",
    "\n",
    "The DataFrame should have rows for each cluster group 0, 1, 2, 3 and columns for the mean statistics.  \n",
    "\n",
    "Add a column `Num` reporting the number of samples in each group. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5c2a-6fdb-4113-9217-6515a948bb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Data Frame for the mean statistics of each group \n",
    "\n",
    "\n",
    "clusterStats[['Num', 'MP', 'FG', '3P', 'FT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959bbeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8625e-0bd0-45dd-ba69-0ab35fe92de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "x7468csSIxk6"
   },
   "source": [
    "## Q2(e)\n",
    "\n",
    "Report the same statistics as in (e), but using the original data scaling (reverse the scaling back to the original data range). \n",
    "\n",
    "Store results in `clusterStatsOrig`; this DataFrame should not have the \"Num\" column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41e5f6-3f48-4b6b-b859-f0157bc5e7b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Ghj4SEpXHzAV",
    "outputId": "64fbf9fd-a15c-4252-b529-43796b2f4b77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Data Frame for the mean statistics of each group (using the \n",
    "#   original data scaling)\n",
    "\n",
    "\n",
    "clusterStatsOrig = ...\n",
    "\n",
    "clusterStatsOrig[['MP', 'FG', '3P', 'FT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1e08d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af88e03-8d64-431d-bce9-ea07a6e35661",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QIwBbzbpJQq-"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2(f) \n",
    "\n",
    "Apply PCA to the basketball data.  Plot the first two principal components, colored by the best group labels found above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067f66a-5cd7-4d05-a071-11d4a46c268b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "YC1HZ7yAJSo0",
    "outputId": "d7c066b4-ca40-4beb-defb-e3fd3e5769b6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run PCA on the nba data and plot the first two principal components\n",
    "#  colored by the group labels. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b993e-f8c8-44a2-b7d7-b0b5c544cd7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q3 : Clustering - Spotify Music\n",
    "\n",
    "For this problem you will look at popular streaming music.  Specifically, Spotify's top 100 streaming songs.  For each song information about the song is described with different properties: `duration`, `energy`, `key`, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac3933-ef48-44d1-b717-49443b2f10f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(a) - Load and Prepare the Data \n",
    "\n",
    "Load in the `music.csv` data.  \n",
    "\n",
    "The clustering algorithms will only consider variables of `duration` to the end of the DataFrame. \n",
    "\n",
    "Standardize the variables to be used in clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7953696-78ae-4414-8f08-fee696e89bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in music data \n",
    "\n",
    "music = pd.read_csv(...)\n",
    "\n",
    "music.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80922ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168d5da-71af-4cc9-a4d0-e83706d4f4a8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(b) - Hierarchical Clustering \n",
    "\n",
    "Perform Hierarchical clustering with **single** linkage on just the top 30 songs. \n",
    "\n",
    "Report results in a dendrogram, `dg_single` and label the samples by the Artist.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842020bb-b97f-42d3-be42-a0af6c745cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Hierarchical clustering with single linkage on top 30 songs \n",
    "# Report results in a dendrogram, dg_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48264f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25137c87-0094-4628-a42f-22a40cd0d3c2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(c) - Hierarchical Clustering, part 2 \n",
    "\n",
    "Perform Hierarchical clustering with **complete** linkage on just the top 30 songs.\n",
    "\n",
    "Report results in a dendrogram, `dn_complete` and label the samples by the Artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8169b7a-8492-44fe-8328-3a5298707ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Hierarchical clustering with complete linkage on top 30 songs \n",
    "# Report results in a dendrogram, dg_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8222e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea1262-05af-47aa-ad6a-d5940df8e23b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(d) - Hierarchical Clustering, part 3\n",
    "\n",
    "Perform Hierarchical clustering with **aveage** linkage on just the top 30 songs.\n",
    "\n",
    "Report results in a dendrogram, `dn_average` and label the samples by the Artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae46a20-0a02-4918-8d72-3a6c82139060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Hierarchical clustering with average linkage on top 30 songs \n",
    "# Report results in a dendrogram, dg_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6855",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69550bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "**NOTE** the submission must be run on the campus linux machines.  See the instruction in the Canvas assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec07769",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec841d03",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cs4821] *",
   "language": "python",
   "name": "conda-env-.conda-cs4821-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "a4",
   "tests": {
    "q0": {
     "name": "q0",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> COLAB == False\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (LLM == True) | (GS == True)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": [
      2,
      2,
      2,
      2,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(textdata) == 327\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.bincount(yvalues) == [119, 208])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> textdata[0].decode()[0:40] == '\\r\\ni muse my lord of gloucester is not co'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> textdata[10].decode()[0:40] == '\\r\\nkill the poys and the luggage tis expr'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> textdata[23].decode()[200:250] == ' bishop here at whose hands\\r\\nhe hath good usage an'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> textdata[13].decode()[50:100] == 'ir give me your hand sir an early stirrer by\\r\\nthe '\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": [
      2,
      2,
      2,
      2,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(text_trainval) == 261\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(text_test) == 66\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(y_trainval[0:10] == [1, 0, 1, 0, 0, 1, 1, 0, 0, 1])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(y_test[0:10] == [0, 0, 1, 0, 1, 1, 1, 1, 1, 1])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> text_trainval[0].decode()[500:530] == 'irds\\r\\nconceive when after many'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> text_test[0].decode()[1000:1040] == 'er than those that are so washed how muc'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dtm_trainval.shape == (261, 13953)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(vocab) == 13953\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(dtm_trainval) == scipy.sparse._csr.csr_matrix\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(vocab[450:460] == ['answer', 'answerable', 'answered', 'answers', 'anthony', 'antic', 'anticly', 'antics', 'antiopa', 'antipholus'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(train_acc_bern, 0.95019157)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(X_trainval[0, 1:10].toarray()[0] == [0, 0, 0, 0, 0, 0, 0, 0, 0])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(X_trainval[5, 1280:1290].toarray()[0] == [0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1g": {
     "name": "q1g",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> train_acc_mult == 1.0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(X_trainval[0, 1:10].toarray()[0] == [0, 0, 0, 0, 0, 0, 0, 0, 0])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(X_trainval[5, 1280:1290].toarray()[0] == [0, 2, 0, 0, 0, 0, 0, 0, 0, 1])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1i": {
     "name": "q1i",
     "points": [
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(grid.best_params_['vec']) == TfidfVectorizer\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1j": {
     "name": "q1j",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(np.isclose(importance[0:10], [0.0, 0.0, 0.00054952, 0.00035462, 0.00024037, 0.0, 0.00077599, 0.00040087, 0.0, 0.0]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": [
      2.5,
      0.5,
      0.5,
      0.5,
      0.5,
      0.5,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nba.shape == (420, 25)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not 'FG%' in nba.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not '3P%' in nba.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not '2P%' in nba.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not 'eFG%' in nba.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> not 'FT%' in nba.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nba.iloc[4:8, 5] == [81, 38, 80, 81])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nba.iloc[100:105, 23] == [2.1, 1.1, 1.3, 2.4, 1.5])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nbaScaled.shape == (420, 18)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": [
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(sse) == 9\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> clusterStats.shape == (4, 19)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2e": {
     "name": "q2e",
     "points": [
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> clusterStatsOrig.shape == (4, 18)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> music.shape == (100, 17)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(music.columns == ['Song', 'Artist', 'Streams (Billions)', 'Release Date', 'id', 'duration', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'danceability'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 9,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dg_single['leaves'][1:5] == [23, 11, 14, 21]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_single['leaves'][20:24] == [8, 19, 25, 4]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_single['leaves'][10:15] == [10, 13, 2, 5, 3]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 9,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dg_complete['leaves'][1:5] == [1, 2, 3, 5]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_complete['leaves'][10:15] == [27, 11, 13, 6, 20]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_complete['leaves'][23:27] == [22, 16, 26, 7]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": 9,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dg_average['leaves'][1:5] == [15, 28, 23, 8]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_average['leaves'][10:15] == [20, 3, 5, 13, 22]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dg_average['leaves'][24:29] == [10, 11, 14, 27, 0]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
